\begin{frame}{Outline}
	\definecolor{prediction}{HTML}{b30000}
	\newcommand{\arrowlenshort}{2.2cm}
	\newcommand{\arrowlen}{2.9cm}
	\hspace*{-1.7cm}
	\begin{tikzpicture}[
			>=stealth,
			block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.2cm, text width=2.3cm,},
			node distance=\arrowlen
		]

		\node[block,align=center] (u) {System\\Discretization};
		\node[block, right=of u, align=center] (sys) {Discretize\\Perf. Index $\mathcal{J}$};\node[block, right=of sys,xshift=-0.7cm, align=center] (dp) {Dynamic\\Programming};
		

		\draw[->] ($(u.west)+(-\arrowlenshort,0.2cm)$) -- ($(u.west)+(0,0.2cm)$)
		node[midway, above] {$\dot{x}= Ax + Bu$};


		\draw[->]
		($(u.east)+(0,0.2cm)$) -- ++(\arrowlen,0)
		node[midway, above] {$x_{k+1}=\overline{A} x_k + \overline{B} u_k$};

		\draw[->]
		($(sys.east)+(0,0.2cm)$) -- ++(\arrowlenshort,0)
		node[midway, above] {$\mathcal{J}_{k}$};

		\draw[->]
		(dp.south)
		-- ++(0,-0.5)        % vertical segment
		-- ++(1.7cm,0)                     % horizontal segment to the right
		node[pos=0.3, align=center, yshift=-5.5mm] {Optimal control $u_k$ \\ with minimal cost $\mathcal{J}$};

		\draw[->]
		(2.8cm,-1.1cm)
		-- (2.8cm, -0.2cm)
		-- ($(sys.west)+(0,-0.2cm)$)
		node[midway, below] {$Q,\, R$};

        \only<1>{   
        \draw[->] ($(u.west)+(-\arrowlenshort,-0.2cm)$) -- ($(u.west)+(0,-0.2cm)$)
				node[midway, below] {$\{ t_0, \, \ldots, t_N \}$};}


        \only<2>{   
        \draw[->, color=prediction,thick] ($(u.west)+(-\arrowlenshort,-0.2cm)$) -- ($(u.west)+(0,-0.2cm)$)
				node[midway, below] {$\{ t_0, \, \ldots, t_N \}$};}
        \uncover<3->{
        \node[block, align=center, below=0.7cm of u,color=prediction,thick] (sample) {Sampling\\ Method};
        \draw[->, color=prediction, thick]
		($(sample.west)+(0,0)$)
		-- ($(sample.west)+(-\arrowlenshort,0)$)  % horizontal line to original start of arrow
		% start at middle of left side of Sampling box
		-- ($(u.west)+(-\arrowlenshort,-0.2cm)$)  % horizontal line to original start of arrow
		-- ($(u.west)+(0,-0.2cm)$)                % original arrow path
		node[midway, below, color=prediction, thick] {$\{ t_0, \, \ldots, t_N \}$};}


	\end{tikzpicture}
    \uncover<4>{
    \begin{itemize}
				\setlength{\itemsep}{\bulletSpacing}
				\item Sampling Density and Sampling Method Cost\\[2mm]
				\item Sampling Methods for finding $\{t_0, \, \ldots, t_N \}$
				      \begin{enumerate}
					      \item Periodic sampling
					      \item Lebesgue sampling
					      \item Quantization-based sampling
				      \end{enumerate}
			\end{itemize}

    }

\note<1>{\begin{itemize}
\item so far, we have seen how given a system and a set of sampling instants, we can discretize everything we had seen in continuous time
\item this includes discretizing the system itself, as well as the perf index
\item once we have this discretization, we can compute the optimal control sequence for those sampled instants $t_0, \ldots, t_N$
\end{itemize}}

\note<2>{\begin{itemize}
\item to actually apply this procedure, we need two things at the beginning: the system, which is straightforward, and the sampling instants
\item up to this point, we assumed the sampling instants were given
\item now, we will focus on how to choose these sampling instants, so that the resulting optimal control (from our pipeline) is as close as possible to the continuous-time optimal control
\item meaning the choice of these sampling instants determines how well the discretized system approximates the original continuous-time system
\end{itemize}}

\note<3>{\begin{itemize}
\item in this part of our lecture, i am going to present different sampling methods to determine these sampling instants
\item each sampling method will give us a different set of sampling instants, which will lead to a different discretized system, and thus a different optimal control and cost
\item to compare the results for these sampling methods, we will introduce two concepts: sampling density and normalized cost

\end{itemize}}

\note<4>{\begin{itemize}

\item sampling density captures how the sampling instants are distributed over time
\item normalized cost captures how close the cost obtained with a given sampling method is to the optimal cost of the continuous-time system
\item we will define both concepts in the next slides
\item after that, we will present three different sampling methods: periodic sampling, lebesgue sampling, and quantization-based sampling
\item for each method, we will discuss how it works, and we will analyze its sampling density and normalized cost
\end{itemize}}

\end{frame}


\begin{frame}[t]{Comparing Sampling Methods: Sampling Density}
    \vspace{-0.5cm}

	\begin{block}{Sampling Density}
		Given $x_0$, $A$, $B$, $Q$, $R$, and $S$, and interval length $T$, and a number of samples $N$, the \emph{sampling density} $\sigma_{N,m}:[0,T] \to \mathbb{R}^{+}$ of any sampling method $m$ is defined as
		\[
			\sigma_{N,m}(t) = \frac{1}{N\,\tau_{k}} \quad \forall t \in [t_k, t_{k+1}), \quad k \in \{0, \ldots, N-1\}
		\]
		\vspace{-1em}
		\begin{itemize}
    \item Sampling density is normalized \pause \, \textcolor{gray}{\scriptsize  $\displaystyle\int_0^T \sigma_{N,m}(t)\, dt = \sum_{k=0}^{N-1} \int_{t_k}^{t_{k+1}} \sigma_{N,m}(t) \, dt = \sum_{k=0}^{N-1} \frac{1}{N \tau_k}\tau_k = 1$}
\end{itemize}
        
	\end{block}
    \vspace{-0.3cm}

    \begin{overlayarea}{\textwidth}{6cm}
    \only<3>{
    \begin{block}{Asymptotic Sampling Density}
			To remove the dependency on $N$, we define the \emph{asymptotic sampling density} as $\sigma_{m}:[0,T] \to \mathbb{R}^{+}$ as the limit
			\[
				\sigma_{m}(t) = \lim_{N \to \infty} \sigma_{N,m}(t)
			\]
		\end{block}
    }
	
	\uncover<4->{\begin{example}[$T=5$, $N=4$]
			
        \begin{tabular}{@{}p{0.5\linewidth}@{} p{0.5\linewidth}@{}}
            \hspace{-0.1cm}
            \vspace{-0.6cm}

            \includegraphics[width=\linewidth]{plots/density_plot.pdf}
            &
            \uncover<5>{
            \vspace{-0.3cm}
            \begin{itemize}
                \item $\sigma_4\,(t) = \frac{1}{4\,\cdot\,1} = \frac{1}{4}, \quad \forall t \in [0,1)$\\[0.3cm]
                \item $\sigma_4\,(t) = \frac{1}{4\,\cdot\,3} = \frac{1}{12}, \quad \forall t \in [1,4)$
            \end{itemize}
            }
            
        \end{tabular}

		\end{example}
    }
    \end{overlayarea}

\note<1>{\begin{itemize}
\item as mentioned earlier, sampling density tells us how the sampling instants are distributed over time
\item we divide the interval $[0,T]$ into $N$ subintervals $$[t_k, t_{k+1})$$ based on the sampling instants
\item now sampling density is defined within each subinterval 
\item it is defined as the inverse of the product of the number of samples and the interarrival time
\item this means that if the interarrival time is small (smaller subintervals), the sampling density is high, indicating that we are sampling more frequently in that interval
\item conversely, if the interarrival time is large (larger subintervals), the sampling density is low, indicating that we are sampling less frequently in that interval
\item the sampling density is normalized by dividing by $N$, meaning that the integral of the sampling density over the entire
time interval equals 1
\item this allows comparability of the values across different numbers of samples
\item this normalization ensures that the sampling density represents a valid probability distribution over the time interval
\end{itemize}}

\note<2>{\begin{itemize}
\item up to now sampling density depended on the number of samples N
\item to remove this dependency, we define the asymptotic sampling density as the limit of the sampling density as N approaches infinity
\item think of it as the idealized sampling density we would get if we could sample infinitely often
\item so the ideal distribution of sampling instants over time
\end{itemize}}

\note<3>{\begin{itemize}
\item here is an example to illustrate the concept of sampling density
\item consider a time interval of length T=5, and we choose N=4 sampling instants
\item let's say we choose the sampling instants as t0=0, t1=1, t2=4, and t3=5
\item this divides the interval [0,5] into 3 subintervals: [0,1), [1,4), and [4,5)
\item now we can compute the sampling density for each subinterval
\item for the first subinterval [0,1), the interarrival time is 1-0=1, so the sampling density is 1/(4*1)=1/4
\item for the second subinterval [1,4), the interarrival time is    4-1=3, so the sampling density is 1/(4*3)=1/12
\item you can see that the sampling density is higher in the first subinterval compared to the second subinterval, reflecting the fact that we are sampling more frequently in the first interval 
\item Übergang: so far we have talked about where the sampling instants are placed in time, now we will look at how well the sampling method approximates the continuous-time optimal control in terms of cost
\end{itemize}}

\end{frame}

\questionslide{Is it okay that the derivation for sampling density being normalized is shown afterwards or should it also be shown directly? Right now it also stays for the following slides, should the item for the normalized disappear afterwards?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Normalized Cost %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Comparing Sampling Methods: Normalized Cost}

	\begin{block}{Normalized Cost}
		Given $x_0$, $A$, $B$, $Q$, $R$, and $S$, and interval length $T$, and a number of samples $N$, the \emph{normalized cost} of any sampling method $m$ is defined as
		\[
			c_{N,m} =\frac{N^2}{T^2} \frac{\mathcal{J}_{N,m} - \mathcal{J}_\infty}{\mathcal{J}_\infty}
		\]
		where $\mathcal{J}_{N,m}$ is the minimal cost of the sampling method $m$ with $N$ samples, and $\mathcal{J}_\infty$ is the minimal cost of the continuous-time system.
	\end{block}

	\uncover<2->{
    \begin{block}{Asymptotic Normalized Cost}
		To remove the dependency on $N$, we define the \emph{asymptotic normalized cost} as the limit
		\[
			c_{m} = \lim_{N \to \infty} c_{N,m}
		\]
	\end{block}
    }
\note<1>{\begin{itemize}
\item normalized cost measures how close the cost obtained with a given sampling method is to the optimal cost of the continuous-time system
\item for a fixed number of samples $N$ and a sampling method $m$ leads to a discrete time optimal control problem with minimal cost $\mathcal{J}_{N,m}$
\item we want to compare this to the ideal cost we can get, which is the optimal cost $\mathcal{J}_{\infty}$ of the continuous-time system
\item so this serves as our baseline
\item the difference $\mathcal{J}_{N,m} - \mathcal{J}_\infty$ tells us how much worse the sampled cost is compared to the optimal continuous-time cost
\item dividing by $\mathcal{J}_\infty$ normalizes this difference, giving us a relative measure of how much worse the sampled cost is compared to the optimal continuous-time cost, this makes it comparable across different systems and cost scales
\item this difference is then scaled by the square of the number of samples divided by the square of the total time interval
\item this scaling accounts for the fact that if we can achieve same cost with fewer samples, we want a penalty for taking more samples
\item we divide by $T^2$ to normalize this scaling with respect to the length of the time interval
\item thus, a lower normalized cost indicates that the sampling method is more effective in approximating the continuous-time optimal control
\end{itemize}}

\note<2>{\begin{itemize}
\item similar to sampling density, normalized cost also depends on the number of samples N
\item to remove this dependency, we define the asymptotic normalized cost as the limit of the normalized cost as N approaches infinity
\item this gives us a measure of how well the sampling method performs in the idealized case of infinite sampling
\item so $c_m$ provides a single scalar value that allows us to compare samplig methods independently of $N$
\end{itemize}}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Comparing Sampling Methods: Normalized Cost}	
\vspace{-0.4cm}

\begin{example}[Normalized Cost for Periodic Sampling]
\begin{minipage}[t][6.5cm][t]{\linewidth}
\only<1>{
\hspace*{-0.04\linewidth}
\begin{tabular}{>{\centering\arraybackslash}p{0.5\linewidth} >{\centering\arraybackslash}p{0.5\linewidth}}
    $N=10$ & $N=60$ \\
    \includegraphics[width=\linewidth]{./plots/pointwise_N=10.pdf} &
    \includegraphics[width=\linewidth]{./plots/pointwise_N=60.pdf} \\[0.5mm]
    
    \begin{itemize}
        
        \item $\mathcal{J}_{\infty}=383.2$,\, $\mathcal{J}_{10,\text{per}}=699.7$\\[0.2cm]
    
        \item $c_{10,\text{per}}=\frac{10^2}{6^2} \frac{699.7 - 383.2}{383.2}=2.3$
        
    \end{itemize} &
    \begin{itemize}
        \item $\mathcal{J}_{\infty}=383.2$, \, $\mathcal{J}_{60,\text{per}}=385.5$\\[0.2cm]
        
        \item $c_{60,\text{per}}= \frac{60^2}{6^2} \frac{385.5 - 383.2}{383.2}=0.6$
        
    \end{itemize}
    
    
\end{tabular}
}
    \only<2>{
    \centering
    \includegraphics[height=6.5cm]{./plots/normalized_cost_vs_N.pdf} 

}
\end{minipage}
\end{example}
\note<1>{\begin{itemize}
\item let's look at an example to illustrate the concept of normalized cost
\item we consider periodic sampling with two different numbers of samples: $N=10$ and $N=60$
\item in black see again continuous control and in blue the piecewise constant control obtained with periodic sampling
\item every $0.6$s and every $0.1$s respectively control is updated
\item right plot you see that continous control is approximated much better with N=60 then with N=10
\item in both cases, the continuous-time optimal cost is the same $383.2$, since the underlying system does not change
\item with only $10$ samples, the discretized problem has resulting cost of $699.7$, which is much larger than the continuous-time optimum
\item when we increase the number of samples to $60$, the cost of the discretized problem decreases to $385.5$, which is much closer to the continuous-time optimum
\item using these values, we can compute the normalized cost for N=10, which is 2.3
\item this indicates that with 10 samples, the cost of the sampled system is significantly higher than the optimal continuous-time cost
\item using these values, we compute the normalized cost for N=60, which is 0.6
\item this indicates that with 60 samples, the cost of the sampled system is much closer to the optimal continuous-time cost
\item we can see that as we increase the number of samples from 10 to 60, the normalized cost decreases from 2.3 to 0.6
\item this shows that increasing the number of samples improves the performance of the sampling method in approximating the continuous-time optimal control
\item Übergang: now we have looked at two chosen values of $N$, we want to see how the normalized cost behaves for different values of $N$
\end{itemize}}

\note<2>{\begin{itemize}
\item in this plot, we see how the normalized cost for periodic sampling behaves as we vary the number of samples N
\item start at $N=10$ as for lower number the cost explodes because we have seen too less
\item as we can see, as we increase the number of samples, the normalized cost decreases
\item this indicates that with more samples, the cost of the sampled system gets closer to the optimal continuous-time cost
\item this trend suggests that periodic sampling becomes more effective in approximating the continuous-time optimal control as we increase the number of samples
\item however we see that after $N=100$ the normalized cost starts to increase again
\item so sampling further no longer improves the approximation
\item similar results can be achieved with fewer samples, so increasing N yields to higher normalized cost as a penalty
\item Übergang: now that we have defined the concepts of sampling density and normalized cost, we can look at specific sampling methods and analyze their properties
\end{itemize}}


\end{frame}

\questionslide{Is it okay that we started at $N=10$ and also the way we denoted it in the graph, or is the x axis too chaotic?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Periodic Sampling %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Periodic Sampling}
	We divide the interval $[0,T]$ into $N$ parts of equal size
	\[
\begin{aligned}
\tau_k &= \tau = \frac{T}{N}, 
& k &\in \{0,\ldots,N-1\}, \\[4pt]
t_k &= k\,\tau = k\,\frac{T}{N}, 
& k &\in \{0,\ldots,N\}.
\end{aligned}
\]

\pause
	For $N \in \mathbb{N}$, we get the constant sampling density
	\[
		\sigma_{per, N}(t) = \frac{1}{N\cdot \tau_k}
		= \frac{1}{N}\cdot \frac{N}{T}
		= \frac{1}{T}
	\]

\note<1>{\begin{itemize}
\item now we will look at periodic sampling as our first sampling method
\item in periodic sampling, we divide the time interval $[0,T]$ into $N$ subintervals of equal length
\item this means that the interarrival time between consecutive sampling instants is constant and equal to $\frac{T}{N}$
\item therefore also the sampling instants are equally spaced and given by multiplying the interarrival time by the index $k$
\item we start at $t_0=0$ ($k=0$) and advance in equal steps of size $\tau$ until we reach $t_N=T$
\item after $k$ such steps, we reach the sampling instant $t_k=k\cdot \tau$
\item this results in a uniform distribution of sampling instants over the time interval
\end{itemize}}

\note<2>{\begin{itemize}
\item now let's compute the sampling density for periodic sampling
\item since the interarrival time is constant (all intervals have equal length), the sampling density is also constant over the entire time interval
\item substituting the interarrival time into the sampling density formula, we find that the sampling density for periodic sampling is $\frac{1}{T}$
\item this means that the sampling density does not depend on the number of samples $N$, and is uniform over the time interval
\item this reflects the fact that in periodic sampling, we sample at a constant rate throughout the entire time interval
\item Übergang: now that we have analyzed the sampling density of periodic sampling, we will look at its normalized cost
\item to do this, we first need to examine the optimal control obtained with periodic sampling, which requires solving the discrete-time Riccati equation to get $\mathcal{J}_{N,m}$
\item as we have constant interarrival times, this solution can actually be obtained analytically
\end{itemize}}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Optimal Control}
	For sampling period $\tau$, the solution $\overline{K}\,(\tau)$ of the discrete-time Riccati equation can be determined analytically as

	\[
		\overline{K}\,(\tau) = K_{\infty} + X \cdot \frac{\tau^2}{2} + o(\tau^2) \quad [1]
	\] 

	where $K_{\infty}$ is the solution of the Riccati equation in continuous-time, and $X$ is the solution of a Lyapunov equation\footnote{}


	\bigskip
    \uncover<2>{
	\begin{columns}
		\begin{column}{0.7\textwidth}

			{
				\color{gray!80}
				\textit{
					$\ldots$ informally, optimal controller of the discrete-time can be expressed as the continuous-time solution $K_\infty$ plus a correction term that is proportional to the square of the sampling period $\tau$.
				}
			}
		\end{column}
	\end{columns}
    }

    \vspace{0.9cm}
	\begin{footnotesize}
		$^1$ Melzer, Stuart M., and Benjamin C. Kuo. "Sampling period sensitivity of the optimal sampled \\[0.1mm]
        data linear regulator." Automatica 7.3 (1971): 367-370.
	\end{footnotesize}

\note<1>{\begin{itemize}
\item for a given sampling period $\tau$, the solution of the discrete-time riccati equation can be expressed in closed form
\item the solution $\overline{K}(\tau)$ can be expressed as the sum of the continuous-time solution $K_{\infty}$ and a correction term that is proportional to the square of the sampling period $\tau$
\item the term $X$ is the solution of a Lyapunov equation, which captures the effect of the sampling period on the controller
\item the $o(\tau^2)$ term represents higher-order terms that become negligible as $\tau$ approaches zero
\item the reference for this result is provided by melzer and kuo (paper just used it as given)
\item Übergang: now that we have the expression for the optimal control with periodic sampling, we can use this to analyze the normalized cost
\end{itemize}}

\note<2>{\begin{itemize}
\item informally, this result tells us that the optimal controller of the discrete-time system can be expressed as the continuous-time solution $K_\infty$ plus a correction term that is proportional to the square of the sampling period $\tau$
\item  this expression shows that as the sampling period $\tau$ decreases, the solution of the discrete-time Riccati equation approaches the continuous-time solution $K_{\infty}$
\end{itemize}}

\end{frame}

% Question slide
\questionslide{
	Should there be a [1] in the equation, or only after Lyapunov equation?
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Asymptotic Normalized Cost}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{equationbox}[1\textwidth]
				c_{N,\text{per}} =\frac{N^2}{T^2} \frac{\mathcal{J}_{N,\text{per}} - \mathcal{J}_\infty}{\mathcal{J}_\infty}
			\end{equationbox}
		\end{column}


		\begin{column}{0.5\textwidth}
			\begin{equationbox}[1\textwidth]
				\overline{K}\,(\tau) = K_{\infty} + X \cdot \frac{\tau^2}{2} + o(\tau^2)
			\end{equationbox}
		\end{column}
	\end{columns}

	\newcommand{\LineSpacing}{0.4mm}
	{\footnotesize	\begin{eqnarray*}
			c_{\text{per}} & = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \overline{K}(\tau)\, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
            \pause
			& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \left( K_\infty + X\cdot \frac{\tau^2}{2} + o(\tau^2) \right) \, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
            \pause
			& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \left( K_\infty + X\cdot \frac{T^2}{2N^2} + o\left(\frac{T^2}{2N^2}\right) \right) \, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
            \pause
			& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_0' \,\frac{XT^2}{2N^2} \, x_0 + x_0' \,o\left(N^{-2}\right)\,x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
            \pause
			& = & \frac{x_0' \, X \, x_0}{2\, x_0'\, K_{\infty} \, x_0}
		\end{eqnarray*}
	}

\note{\begin{itemize}
\item on the left we recall formula for normalized cost and on right the expression for the controller
\item we start with the definition of the normalized cost for periodic sampling
\item we substitute the expression for $\overline{K}(\tau)$ into the normalized cost formula
\item we then express the sampling period $\tau$ in terms of the number of sampling intervals $N$
\item ... explain step by step
\item solution shows that the asymptotic normalized cost converges to a constant
\item can be computed using the initial state $x_0$, the continuous-time Riccati solution $K_{\infty}$, and the solution $X$ of the Lyapunov equation
\item it is independent of the number of samples $N$
\item Übergang: as we have derived general expression for the asymptotic normalized cost, let's look at a concrete example 

\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Example}
	\centering
    \vspace{-0.5cm}
	\begin{equationbox}[4cm]
		c_{\text{per}} = \frac{x_0' \, X \, x_0}{2\, x_0'\, K_{\infty} \, x_0}
	\end{equationbox}

	\begin{example}
		For a first-order system $(n=1)$, wlog\ $B = R = 1$, we obtain
		\[
			\begin{aligned}
			K_\infty & = A + \sqrt{A^2 + Q}, \\
			X        & = \frac{1}{12} (K_\infty - A) K_\infty^2
		\end{aligned}
		\]

		which gives us the asymptotic normalized cost
		\[
			c_{\text{per}} = \frac{1}{24} A \sqrt{A^2 + Q} + A^2 + Q .
		\]
	\end{example}

\note{\begin{itemize}
\item consider a first-order system, where we can assume without loss of generality that $B=R=1$
\item this is because we can rescale the control input accordingly
\item in this case, we can derive explicit expressions for both $X$ and $K_{\infty}$
\item we first compute $K_{\infty}$ using the continuous-time algebraic Riccati equation
\item for a first order system, this simplifies to the expression shown
\item next, we compute $X$ 
\item substituting these expressions into the formula for the asymptotic normalized cost, we obtain a closed-form expression for $c_{\text{per}}$
\item this expression shows how the asymptotic normalized cost depends on the system parameters $A$ and $Q$
\item Intuition: $A$ captures the system dynamics, while $Q$ captures the state cost
\item $A$ is how unstable/fast system is, larger $A$ means state evolves more between samples, so higher cost for sampling
\item $Q$ is how much we care about state deviations, larger $Q$ means that errors are penalized more, so higher cost for sampling
\item Übergang: now that we have analyzed periodic sampling, we will move on to our next sampling method: lebesgue sampling
\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Lebesgue Sampling %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Deterministic Lebesgue Sampling}

	\begin{itemize}
		\item \textbf{Intuition:} Sample more frequently where the control changes faster
		\item Sample whenever the optimal $u$ changes by a fixed threshold $\Delta$, so after any sampling instant $t_k$, the next $t_{k+1}$ is determined s.t.
		      $$
			      \left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
		      $$

		      where $u$ is the optimal continuous-time input
	\end{itemize}

	\begin{example}[$\Delta=7$]			
        \centering
        \includegraphics[height=3cm]{plots/lebesgue_plot.pdf} 
	\end{example}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Deterministic Lebesgue Sampling: Example}
    \centering
    \vspace{-0.3cm}
	\begin{equationbox}[6cm]
		\left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
	\end{equationbox}
    \vspace{-0.2cm}
	\begin{example}[$\Delta=7$]			
        \centering
        \includegraphics[height=5.4cm]{plots/lebesgue_plot.pdf} 
	\end{example}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Deterministic Lebesgue Sampling: Deriving the Relation for $N$}

	\centering

	\begin{equationbox}[6cm]
		\left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
	\end{equationbox}
        \bigskip

   {\raggedright
In the case of a scalar input ($m=1$) and a given number $N$ of sampling instances in $[0,T]$, the sampling instants $t_k$ satisfy
\par}
	\newcommand{\LineSpacing}{0mm}

	\begin{eqnarray*}
		\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt & = & |\,u\,(t_{k+1}) - u\,(t_{k})\,| \, = \, \Delta \\[\LineSpacing]
		& = & \frac{1}{N} \cdot \underbrace{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt}_{N\cdot\Delta}
	\end{eqnarray*}

\end{frame}

\begin{frame}{Deterministic Lebesgue Sampling: Sampling Density}

	\centering
	\begin{equationbox}[8cm]
		\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt =
		\frac{1}{N} \cdot {\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt}
	\end{equationbox}
    \vspace{-0.6cm}
	{\Large\[
		\begin{array}{>{\displaystyle}rcl}
			\sigma_{\text{dls}}\, (t) & = & \frac{\vphantom{ |\,\dot{u}(t)\,|\,dt}\,1}{N\cdot\tau_k}                                        
            \, = \, \frac{\int_{t_k}^{t_{k+1}} |\,\dot{u}(t)\,|\,dt}
     {\int_{0}^T |\,\dot{u}(t)\,|\,dt}
\cdot
\frac{\vphantom{ |\,\dot{u}(t)\,|\,dt}\,1}{\tau_k} \\[5mm]
            & = & \frac{\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt}{\vphantom{A^A}\tau_k} \cdot \frac{\vphantom{ |\,\dot{u}(t)\,|\,dt}\,1}{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt} \\[5mm]
            & = & \frac{\vphantom{A_{A^0}} |\,u\,(t_{k+1}) - u\,(t_{k})\,|}{\vphantom{A^A}t_{k+1} - t_k} \cdot \frac{\vphantom{ |\,\dot{u}(t)\,|\,dt}\,1}{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt}\\[5mm]
            &=& \frac{\vphantom{A_{A^0}}|\, \dot{u}\,(t) \,|}{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt}
		\end{array}
	\]}

	% 	\newcommand{\LineSpacing}{-0mm}
	% {\tiny\begin{eqnarray*}
	%     \sigma_{\text{dls}}\, (t) & = & \frac{1}{N\cdot\tau_k} \\[\LineSpacing]
	%                               & = & \frac{\displaystyle\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt}{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt} \cdot \frac{1}{\tau_k} \\[\LineSpacing]
	%                               & = & \frac{\displaystyle\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt}{\tau_k} \cdot \frac{1}{\displaystyle\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt} \\[\LineSpacing]
	%                               & = & \frac{|\,u\,(t_{k+1}) - u\,(t_{k})\,|}{t_{k+1} - t_k} \cdot \frac{1}{\int_{0}^T \, |\, \dot{u}\,(t) \,| \, dt}
	% \end{eqnarray*}
	% }

	% \begin{itemize}
	% 	\item Sampling intervals vary depending on $|\,\dot{u \, (t)}\,|$
	% 	\item No closed formula for the asymptotic normalized cost $c_{\text{dls}}$
	% \end{itemize}

\end{frame}

