\begin{frame}{Outline}
	\definecolor{prediction}{HTML}{b30000}
	\newcommand{\arrowlenshort}{2.2cm}
	\newcommand{\arrowlen}{2.9cm}
	\hspace*{-1.7cm}
	\begin{tikzpicture}[
			>=stealth,
			block/.style={draw, rectangle, minimum height=1.2cm, minimum width=2.2cm, text width=2.3cm,},
			node distance=\arrowlen
		]

		\node[block,align=center] (u) {System\\Discretization};
		\node[block, right=of u, align=center] (sys) {Discretize\\Perf. Index $\mathcal{J}$};\node[block, right=of sys,xshift=-0.7cm, align=center] (dp) {Dynamic\\Programming};


		\draw[->] ($(u.west)+(-\arrowlenshort,0.2cm)$) -- ($(u.west)+(0,0.2cm)$)
		node[midway, above] {$\dot{x}= Ax + Bu$};


		\draw[->]
		($(u.east)+(0,0.2cm)$) -- ++(\arrowlen,0)
		node[midway, above] {$x_{k+1}=\overline{A} x_k + \overline{B} u_k$};

		\draw[->]
		($(sys.east)+(0,0.2cm)$) -- ++(\arrowlenshort,0)
		node[midway, above] {$\mathcal{J}_{k}$};

		\draw[->]
		(dp.south)
		-- ++(0,-0.5)        % vertical segment
		-- ++(1.7cm,0)                     % horizontal segment to the right
		node[pos=0.3, align=center, yshift=-5.5mm] {Optimal control $u_k$ \\ with minimal cost $\mathcal{J}$};

		\draw[->]
		(2.8cm,-1.1cm)
		-- (2.8cm, -0.2cm)
		-- ($(sys.west)+(0,-0.2cm)$)
		node[midway, below] {$Q,\, R$};

		\only<1>{
			\draw[->] ($(u.west)+(-\arrowlenshort,-0.2cm)$) -- ($(u.west)+(0,-0.2cm)$)
			node[midway, below] {$\{ t_0, \, \ldots, t_N \}$};}


		\only<2>{
			\draw[->, color=prediction,thick] ($(u.west)+(-\arrowlenshort,-0.2cm)$) -- ($(u.west)+(0,-0.2cm)$)
			node[midway, below] {$\{ t_0, \, \ldots, t_N \}$};}
		\uncover<3->{
			\node[block, align=center, below=0.7cm of u,color=prediction,thick] (sample) {Sampling\\ Method};
			\draw[->, color=prediction, thick]
			($(sample.west)+(0,0)$)
			-- ($(sample.west)+(-\arrowlenshort,0)$)  % horizontal line to original start of arrow
			% start at middle of left side of Sampling box
			-- ($(u.west)+(-\arrowlenshort,-0.2cm)$)  % horizontal line to original start of arrow
			-- ($(u.west)+(0,-0.2cm)$)                % original arrow path
			node[midway, below, color=prediction, thick] {$\{ t_0, \, \ldots, t_N \}$};}


	\end{tikzpicture}
	\uncover<4>{
		\begin{itemize}
			\setlength{\itemsep}{\bulletSpacing}
			\item Sampling Density and Sampling Method Cost\\[2mm]
			\item Sampling Methods for finding $\{t_0, \, \ldots, t_N \}$
			      \begin{enumerate}
				      \item Periodic sampling
				      \item Lebesgue sampling
				      \item Quantization-based sampling
			      \end{enumerate}
		\end{itemize}

	}

	\note<1>{\begin{itemize}
			\item so far, we have seen how given a system and a set of sampling instants, we can discretize everything we had seen in continuous time
			\item this includes discretizing the system itself, as well as the perf index
			\item once we have this discretization, we can compute the optimal control sequence for those sampled instants $t_0, \ldots, t_N$
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item to actually apply this procedure, we need two things at the beginning: the system, which is straightforward, and the sampling instants
			\item up to this point, we assumed the sampling instants were given
			\item now, we will focus on how to choose these sampling instants, so that the resulting optimal control (from our pipeline) is as close as possible to the continuous-time optimal control
			\item meaning the choice of these sampling instants determines how well the discretized system approximates the original continuous-time system
		\end{itemize}}

	\note<3>{\begin{itemize}
			\item in this part of our lecture, i am going to present different sampling methods to determine these sampling instants
			\item each sampling method will give us a different set of sampling instants, which will lead to a different discretized system, and thus a different optimal control and cost
			\item to compare the results for these sampling methods, we will introduce two concepts: sampling density and normalized cost

		\end{itemize}}

	\note<4>{\begin{itemize}

			\item sampling density captures how the sampling instants are distributed over time
			\item normalized cost captures how close the cost obtained with a given sampling method is to the optimal cost of the continuous-time system
			\item we will define both concepts in the next slides
			\item after that, we will present three different sampling methods: periodic sampling, lebesgue sampling, and quantization-based sampling
			\item for each method, we will discuss how it works, and we will analyze its sampling density and normalized cost
		\end{itemize}}

\end{frame}


\begin{frame}[t]{Comparing Sampling Methods: Sampling Density}
	\vspace{-0.5cm}

	\begin{block}{Sampling Density}
		Given $x_0$, $A$, $B$, $Q$, $R$, and $S$, and interval length $T$, and a number of samples $N$, the \emph{sampling density} $\sigma_{N,m}:[0,T] \to \mathbb{R}^{+}$ of any sampling method $m$ is defined as
		\[
			\sigma_{N,m}(t) = \frac{1}{N\,\tau_{k}} \quad \forall t \in [t_k, t_{k+1}), \quad k \in \{0, \ldots, N-1\}
		\]
		\vspace{-1em}
		\begin{itemize}
			\item Sampling density is normalized \pause \, \textcolor{gray}{\scriptsize  $\displaystyle\int_0^T \sigma_{N,m}(t)\, dt = \sum_{k=0}^{N-1} \int_{t_k}^{t_{k+1}} \sigma_{N,m}(t) \, dt = \sum_{k=0}^{N-1} \frac{1}{N \tau_k}\tau_k = 1$}
		\end{itemize}

	\end{block}
	\vspace{-0.3cm}

	\begin{overlayarea}{\textwidth}{6cm}
		\only<3-4>{\begin{example}[$T=5$, $N=4$]

				\begin{tabular}{@{}p{0.5\linewidth}@{} p{0.5\linewidth}@{}}
					\hspace{-0.1cm}
					\vspace{-0.6cm}

					\includegraphics[width=\linewidth]{plots/density_plot.pdf}
					 &
					\only<4>{
						\vspace{-0.3cm}
						\begin{itemize}
							\item $\sigma_4\,(t) = \frac{1}{4\,\cdot\,1} = \frac{1}{4}, \quad \forall t \in [0,1)$\\[0.3cm]
							\item $\sigma_4\,(t) = \frac{1}{4\,\cdot\,3} = \frac{1}{12}, \quad \forall t \in [1,4)$
						\end{itemize}
					}
				\end{tabular}

			\end{example}
		}
		\uncover<5->{
			\begin{block}{Asymptotic Sampling Density}
				To remove the dependency on $N$, we define the \emph{asymptotic sampling density} as $\sigma_{m}:[0,T] \to \mathbb{R}^{+}$ as the limit
				\[
					\sigma_{m}(t) = \lim_{N \to \infty} \sigma_{N,m}(t)
				\]
			\end{block}
		}
	\end{overlayarea}

	\note<1>{\begin{itemize}
			\item as mentioned earlier, sampling density tells us how the sampling instants are distributed over time
			\item we divide the interval $[0,T]$ into $N$ subintervals $[t_k, t_{k+1})$ based on the sampling instants
			\item now sampling density is defined within each subinterval
			\item it is defined as the inverse of the product of the number of samples and the interarrival time
			\item this means that if the interarrival time is small (smaller subintervals), the sampling density is high, indicating that we are sampling more frequently in that interval
			\item conversely, if the interarrival time is large (larger subintervals), the sampling density is low, indicating that we are sampling less frequently in that interval
			\item the sampling density is normalized by dividing by $N$, meaning that the integral of the sampling density over the entire
			      time interval equals 1
			\item this allows comparability of the values across different numbers of samples
			\item this normalization ensures that the sampling density represents a valid probability distribution over the time interval
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item up to now sampling density depended on the number of samples N
			\item to remove this dependency, we define the asymptotic sampling density as the limit of the sampling density as N approaches infinity
			\item think of it as the idealized sampling density we would get if we could sample infinitely often
			\item so the ideal distribution of sampling instants over time
		\end{itemize}}

	\note<3>{\begin{itemize}
			\item here is an example to illustrate the concept of sampling density
			\item consider a time interval of length T=5, and we choose N=4 sampling instants
			\item let's say we choose the sampling instants as t0=0, t1=1, t2=4, and t3=5
			\item this divides the interval [0,5] into 3 subintervals: [0,1), [1,4), and [4,5)
			\item now we can compute the sampling density for each subinterval
			\item for the first subinterval [0,1), the interarrival time is 1-0=1, so the sampling density is 1/(4*1)=1/4
			\item for the second subinterval [1,4), the interarrival time is    4-1=3, so the sampling density is 1/(4*3)=1/12
			\item you can see that the sampling density is higher in the first subinterval compared to the second subinterval, reflecting the fact that we are sampling more frequently in the first interval
			\item Übergang: so far we have talked about where the sampling instants are placed in time, now we will look at how well the sampling method approximates the continuous-time optimal control in terms of cost
		\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Normalized Cost %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Comparing Sampling Methods: Normalized Cost}

	\begin{block}{Normalized Cost}
		Given $x_0$, $A$, $B$, $Q$, $R$, and $S$, and interval length $T$, and a number of samples $N$, the \emph{normalized cost} of any sampling method $m$ is defined as
		\[
			c_{N,m} =\frac{N^2}{T^2} \frac{\mathcal{J}_{N,m} - \mathcal{J}_\infty}{\mathcal{J}_\infty}
		\]
		where $\mathcal{J}_{N,m}$ is the minimal cost of the sampling method $m$ with $N$ samples, and $\mathcal{J}_\infty$ is the minimal cost of the continuous-time system.
	\end{block}

	\uncover<2->{
		\begin{block}{Asymptotic Normalized Cost}
			To remove the dependency on $N$, we define the \emph{asymptotic normalized cost} as the limit
			\[
				c_{m} = \lim_{N \to \infty} c_{N,m}
			\]
		\end{block}
	}
	\note<1>{\begin{itemize}
			\item normalized cost measures how close the cost obtained with a given sampling method is to the optimal cost of the continuous-time system
			\item for a fixed number of samples $N$ and a sampling method $m$ leads to a discrete time optimal control problem with minimal cost $\mathcal{J}_{N,m}$
			\item we want to compare this to the ideal cost we can get, which is the optimal cost $\mathcal{J}_{\infty}$ of the continuous-time system
			\item so this serves as our baseline
			\item the difference $\mathcal{J}_{N,m} - \mathcal{J}_\infty$ tells us how much worse the sampled cost is compared to the optimal continuous-time cost
			\item dividing by $\mathcal{J}_\infty$ normalizes this difference, giving us a relative measure of how much worse the sampled cost is compared to the optimal continuous-time cost, this makes it comparable across different systems and cost scales
			\item this difference is then scaled by the square of the number of samples divided by the square of the total time interval
			\item this scaling accounts for the fact that if we can achieve same cost with fewer samples, we want a penalty for taking more samples
			\item we divide by $T^2$ to normalize this scaling with respect to the length of the time interval
			\item thus, a lower normalized cost indicates that the sampling method is more effective in approximating the continuous-time optimal control
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item similar to sampling density, normalized cost also depends on the number of samples N
			\item to remove this dependency, we define the asymptotic normalized cost as the limit of the normalized cost as N approaches infinity
			\item this gives us a measure of how well the sampling method performs in the idealized case of infinite sampling
			\item so $c_m$ provides a single scalar value that allows us to compare samplig methods independently of $N$
		\end{itemize}}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Comparing Sampling Methods: Normalized Cost}
	\vspace{-0.4cm}

	\begin{example}[Normalized Cost for Periodic Sampling]
		\begin{minipage}[t][6.5cm][t]{\linewidth}
			\only<1>{
				\hspace*{-0.04\linewidth}
				\begin{tabular}{>{\centering\arraybackslash}p{0.5\linewidth} >{\centering\arraybackslash}p{0.5\linewidth}}
					$N=10$                                                                                                                    & $N=60$ \\
					\includegraphics[width=\linewidth]{./plots/pointwise_N=10.pdf}                                                            &
					\includegraphics[width=\linewidth]{./plots/pointwise_N=60.pdf}                                                                     \\[0.5mm]

					\begin{itemize}

						\item $\mathcal{J}_{\infty}=383.2$,\, $\mathcal{J}_{10,\,\text{per}}=699.7$\\[0.2cm]

						\item $c_{10,\,\text{per}}=\frac{10^2}{6^2} \cdot \frac{699.7 - 383.2}{383.2}=2.3$

					\end{itemize} &
					\begin{itemize}
						\item $\mathcal{J}_{\infty}=383.2$, \, $\mathcal{J}_{60,\,\text{per}}=385.5$\\[0.2cm]

						\item $c_{60,\,\text{per}}= \frac{60^2}{6^2} \cdot\frac{385.5 - 383.2}{383.2}=0.6$

					\end{itemize}
				\end{tabular}
			}
			\only<2>{
				\centering
				\includegraphics[height=6.5cm]{./plots/normalized_cost_vs_N.pdf}

			}
		\end{minipage}
	\end{example}
	\note<1>{\begin{itemize}
			\item let's look at an example to illustrate the concept of normalized cost
			\item we consider periodic sampling with two different numbers of samples: $N=10$ and $N=60$
			\item in black see again continuous control and in blue the piecewise constant control obtained with periodic sampling
			\item every $0.6$s and every $0.1$s respectively control is updated
			\item right plot you see that continous control is approximated much better with N=60 then with N=10
			\item in both cases, the continuous-time optimal cost is the same $383.2$, since the underlying system does not change
			\item with only $10$ samples, the discretized problem has resulting cost of $699.7$, which is much larger than the continuous-time optimum
			\item when we increase the number of samples to $60$, the cost of the discretized problem decreases to $385.5$, which is much closer to the continuous-time optimum
			\item using these values, we can compute the normalized cost for N=10, which is 2.3
			\item this indicates that with 10 samples, the cost of the sampled system is significantly higher than the optimal continuous-time cost
			\item using these values, we compute the normalized cost for N=60, which is 0.6
			\item this indicates that with 60 samples, the cost of the sampled system is much closer to the optimal continuous-time cost
			\item we can see that as we increase the number of samples from 10 to 60, the normalized cost decreases from 2.3 to 0.6
			\item this shows that increasing the number of samples improves the performance of the sampling method in approximating the continuous-time optimal control
			\item Übergang: now we have looked at two chosen values of $N$, we want to see how the normalized cost behaves for different values of $N$
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item in this plot, we see how the normalized cost for periodic sampling behaves as we vary the number of samples N
			\item start at $N=10$ as for lower number the cost explodes because we have seen too less
			\item as we can see, as we increase the number of samples, the normalized cost decreases
			\item this indicates that with more samples, the cost of the sampled system gets closer to the optimal continuous-time cost
			\item this trend suggests that periodic sampling becomes more effective in approximating the continuous-time optimal control as we increase the number of samples
			\item however we see that after $N=100$ the normalized cost starts to increase again
			\item so sampling further no longer improves the approximation
			\item similar results can be achieved with fewer samples, so increasing N yields to higher normalized cost as a penalty
			\item Übergang: now that we have defined the concepts of sampling density and normalized cost, we can look at specific sampling methods and analyze their properties
		\end{itemize}}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Periodic Sampling %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Periodic Sampling}
	We divide the interval $[0,T]$ into $N$ parts of equal size
	\[
		\begin{aligned}
			\tau_k & = \tau = \frac{T}{N},
			       & k                           & \in \{0,\ldots,N-1\}, \\[4pt]
			t_k    & = k\,\tau = k\,\frac{T}{N},
			       & k                           & \in \{0,\ldots,N\}
		\end{aligned}
	\]

	\pause
	For $N \in \mathbb{N}$, we get the constant sampling density
	\[
		\sigma_{N,\,\text{per}}(t) = \frac{1}{N\cdot \tau_k}
		= \frac{1}{N}\cdot \frac{N}{T}
		= \frac{1}{T}
	\]

	\note<1>{\begin{itemize}
			\item now we will look at periodic sampling as our first sampling method
			\item in periodic sampling, we divide the time interval $[0,T]$ into $N$ subintervals of equal length
			\item this means that the interarrival time between consecutive sampling instants is constant and equal to $\frac{T}{N}$
			\item therefore also the sampling instants are equally spaced and given by multiplying the interarrival time by the index $k$
			\item we start at $t_0=0$ ($k=0$) and advance in equal steps of size $\tau$ until we reach $t_N=T$
			\item after $k$ such steps, we reach the sampling instant $t_k=k\cdot \tau$
			\item this results in a uniform distribution of sampling instants over the time interval
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item now let's compute the sampling density for periodic sampling
			\item since the interarrival time is constant (all intervals have equal length), the sampling density is also constant over the entire time interval
			\item substituting the interarrival time into the sampling density formula, we find that the sampling density for periodic sampling is $\frac{1}{T}$
			\item this means that the sampling density does not depend on the number of samples $N$, and is uniform over the time interval
			\item this reflects the fact that in periodic sampling, we sample at a constant rate throughout the entire time interval
			\item Übergang: now that we have analyzed the sampling density of periodic sampling, we will look at its normalized cost
			\item to do this, we first need to examine the optimal control obtained with periodic sampling, which requires solving the discrete-time Riccati equation to get $\mathcal{J}_{N,m}$
			\item as we have constant interarrival times, this solution can actually be obtained analytically
		\end{itemize}}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Optimal Control}
	For sampling period $\tau$, the solution $\overline{K}\,(\tau)$ of the discrete-time Riccati equation can be determined analytically as

	\[
		\overline{K}\,(\tau) = K_{\infty} + X \cdot \frac{\tau^2}{2} + o(\tau^2)
	\]

	where $K_{\infty}$ is the solution of the Riccati equation in continuous-time, and $X$ is the solution of a Lyapunov equation\footnote{}


	\bigskip
	\uncover<2>{
		\begin{columns}
			\begin{column}{0.7\textwidth}

				{
					\color{gray!80}
					\textit{
						$\ldots$ informally, optimal controller of the discrete-time can be expressed as the continuous-time solution $K_\infty$ plus a correction term that is proportional to the square of the sampling period $\tau$.
					}
				}
			\end{column}
		\end{columns}
	}

	\vspace{0.9cm}
	\begin{footnotesize}
		$^1$ Melzer, Stuart M., and Benjamin C. Kuo. "Sampling period sensitivity of the optimal sampled \\[0.1mm]
		data linear regulator." Automatica 7.3 (1971): 367-370.
	\end{footnotesize}

	\note<1>{\begin{itemize}
			\item for a given sampling period $\tau$, the solution of the discrete-time riccati equation can be expressed in closed form
			\item the solution $\overline{K}(\tau)$ can be expressed as the sum of the continuous-time solution $K_{\infty}$ and a correction term that is proportional to the square of the sampling period $\tau$
			\item the term $X$ is the solution of a Lyapunov equation, which captures the effect of the sampling period on the controller
			\item the $o(\tau^2)$ term represents higher-order terms that become negligible as $\tau$ approaches zero
			\item the reference for this result is provided by melzer and kuo (paper just used it as given)
			\item Übergang: now that we have the expression for the optimal control with periodic sampling, we can use this to analyze the normalized cost
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item informally, this result tells us that the optimal controller of the discrete-time system can be expressed as the continuous-time solution $K_\infty$ plus a correction term that is proportional to the square of the sampling period $\tau$
			\item  this expression shows that as the sampling period $\tau$ decreases, the solution of the discrete-time Riccati equation approaches the continuous-time solution $K_{\infty}$
		\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Asymptotic Normalized Cost}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{equationbox}[1\textwidth]
				c_{N,\text{per}} =\frac{N^2}{T^2} \frac{\mathcal{J}_{N,\text{per}} - \mathcal{J}_\infty}{\mathcal{J}_\infty}
			\end{equationbox}
		\end{column}


		\begin{column}{0.5\textwidth}
			\begin{equationbox}[1\textwidth]
				\overline{K}\,(\tau) = K_{\infty} + X \cdot \frac{\tau^2}{2} + o(\tau^2)
			\end{equationbox}
		\end{column}
	\end{columns}

	\newcommand{\LineSpacing}{0.4mm}
	\begin{eqnarray*}
		c_{\text{per}} & = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \overline{K}(\tau)\, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
		\pause
		& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \left( K_\infty + X\cdot \frac{\tau^2}{2} + o(\tau^2) \right) \, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
		\pause
		& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_{0}' \, \left( K_\infty + X\cdot \frac{T^2}{2N^2} + o\left(\frac{T^2}{N^2}\right) \right) \, x_0 - x_{0}' \, {K}_{\infty}\, x_0}{x_{0}' \, K_\infty \,x_0} \\[\LineSpacing]
		\pause
		& = & \lim\limits_{N \to \infty} \frac{N^2}{T^2} \, \frac{x_0' \,X\frac{T^2}{2N^2} \, x_0 + x_0' \,o\left(N^{-2}\right)\,x_0}{x_{0}' \, K_\infty \,x_0}
		\pause
		\, = \, \frac{x_0' \, X \, x_0}{2\, x_0'\, K_{\infty} \, x_0}
	\end{eqnarray*}


	\note{\begin{itemize}
			\item on the left we recall formula for normalized cost and on right the expression for the controller
			\item we start with the definition of the normalized cost for periodic sampling
			\item we substitute the expression for $\overline{K}(\tau)$ into the normalized cost formula
			\item we then express the sampling period $\tau$ in terms of the number of sampling intervals $N$
			\item ... explain step by step
			\item solution shows that the asymptotic normalized cost converges to a constant
			\item can be computed using the initial state $x_0$, the continuous-time Riccati solution $K_{\infty}$, and the solution $X$ of the Lyapunov equation
			\item it is independent of the number of samples $N$
			\item Übergang: as we have derived general expression for the asymptotic normalized cost, let's look at a concrete example

		\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Periodic Sampling: Example}
	\centering
	\vspace{-0.5cm}
	\begin{equationbox}[4cm]
		c_{\text{per}} = \frac{x_0' \, X \, x_0}{2\, x_0'\, K_{\infty} \, x_0}
	\end{equationbox}

	\begin{example}
		For a first-order system $(n=1)$, wlog\ $B = R = 1$, we obtain
		\[
			\begin{aligned}
				K_\infty & = A + \sqrt{A^2 + Q},                    \\
				X        & = \frac{1}{12} (K_\infty - A) K_\infty^2
			\end{aligned}
		\]

		which gives us the asymptotic normalized cost
		\[
			c_{\text{per}} = \frac{1}{24} A \sqrt{A^2 + Q} + A^2 + Q .
		\]
	\end{example}

	\note{\begin{itemize}
			\item consider a first-order system, where we can assume without loss of generality that $B=R=1$
			\item this is because we can rescale the control input accordingly
			\item in this case, we can derive explicit expressions for both $X$ and $K_{\infty}$
			\item we first compute $K_{\infty}$ using the continuous-time algebraic Riccati equation
			\item for a first order system, this simplifies to the expression shown
			\item next, we compute $X$
			\item substituting these expressions into the formula for the asymptotic normalized cost, we obtain a closed-form expression for $c_{\text{per}}$
			\item this expression shows how the asymptotic normalized cost depends on the system parameters $A$ and $Q$
			\item Intuition: $A$ captures the system dynamics, while $Q$ captures the state cost
			\item $A$ is how unstable/fast system is, larger $A$ means state evolves more between samples, so higher cost for sampling
			\item $Q$ is how much we care about state deviations, larger $Q$ means that errors are penalized more, so higher cost for sampling
			\item Übergang: now that we have analyzed periodic sampling, we will move on to our next sampling method: lebesgue sampling
		\end{itemize}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Lebesgue Sampling %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Deterministic Lebesgue Sampling}
	\only<1>{
		\begin{itemize}
			\item \textbf{Intuition:} Sample more frequently where the control changes faster
			\item Sample whenever the optimal $u$ changes by a fixed threshold $\Delta$, so after any sampling instant $t_k$, the next $t_{k+1}$ is determined s.t.
			      $$
				      \left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
			      $$

			      where $u$ is the optimal continuous-time input
		\end{itemize}

		\begin{example}[$\Delta=7$]
			\centering
			\includegraphics[height=3cm]{plots/lebesgue_plot.pdf}
		\end{example}
	}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\only<2>{
		\centering
		\vspace{-0.3cm}
		\begin{equationbox}[6cm]
			\left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
		\end{equationbox}
		\vspace{-0.2cm}
		\begin{example}[$\Delta=7$]
			\centering
			\includegraphics[height=5.4cm]{plots/lebesgue_plot.pdf}
		\end{example}
	}


	\note<1>{\begin{itemize}
			\item the intuition behind lebesgue sampling is to sample more frequently in regions where the control changes rapidly
			\item less often when it is relatively constant
			\item instead of sampling at fixed time intervals, we let the behavior of the optimal continuous-time control determine the sampling instants
			\item so of course we first need to calculate the optimal continuous-time control $u(t)$ for $t$ in $[0,T]$
			\item then we set a fixed threshold $\Delta$ that defines how much the control needs to change before we take a new sample
			\item we sample whenever the optimal control input changes by this fixed threshold $\Delta$
			\item this means that after each sampling instant $t_k$, we determine the next sampling instant $t_{k+1}$ such that the norm of the difference between the optimal control at $t_{k+1}$ and $t_k$ equals $\Delta$
			\item so after sampling at $t_k$, we monitor the optimal control $u(t)$ until it changes by $\Delta$, and we set that time as the next sampling instant $t_{k+1}$
			\item this approach allows us to adapt the sampling instants based on the dynamics of the control input
			\item in regions where the control changes rapidly, threshold is reached quickly and we will have more frequent sampling,
			\item in regions where the control changes slowly, takes longer and we will have less frequent sampling
			\item by adjusting $\Delta$, small values lead to more frequent sampling, while larger values result in fewer samples
			\item this adaptive strategy aims to capture the important variations in the control input more effectively (where discretization errors would be larger) than fixed-interval sampling methods
		\end{itemize}}

	\note<2>{\begin{itemize}
			\item here is an example to illustrate the concept of lebesgue sampling
			\item we set the threshold $\Delta$ to 7
			\item the black curve represents the optimal continuous-time control input $u(t)$ over the time interval
			\item the red dots indicate the sampling instants determined by the lebesgue sampling method
			\item as we can see, the sampling instants are not evenly spaced in time
			\item instead, they are concentrated in regions where the control input changes rapidly
			\item in regions where the control input is relatively constant, the sampling instants are more spread out
			\item Übergang: now that we have understood how lebesgue sampling works, we want to analyze its sampling density but for this we first derive an expression relating the number of samples N to the threshold $\Delta$
		\end{itemize}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Deterministic Lebesgue Sampling}
	\only<1>{
		\begin{itemize}
			\item \textbf{Intuition:} Sample more frequently where the control changes faster
			\item Sample whenever the optimal $u$ changes by a fixed threshold $\Delta$, so after any sampling instant $t_k$, the next $t_{k+1}$ is determined s.t.
			      $$
				      \left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
			      $$

			      where $u$ is the optimal continuous-time input
		\end{itemize}

		\begin{example}[$\Delta=7$]
			\centering
			\includegraphics[height=3cm]{plots/lebesgue_plot.pdf}
		\end{example}
	}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\only<2->{
		\centering
		\vspace{-0.4cm}
		\begin{equationbox}[6cm]
			\left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
		\end{equationbox}
		\vspace{-0.2cm}
		\begin{example}[$\Delta=7$]
			\centering
			\begin{tikzpicture}[scale=0.98]
				\newcommand{\DeltaX}{2.58}
				\newcommand{\DeltaLinesX}{2.54} % default 2.54
				\newcommand{\tauplace}{11}
				\input{../template/custom_colors.tex}

				\begin{axis}[
						width=11cm,
						height=6cm,
						axis lines=middle,
						axis line style={line width=1.5pt},
						line width=1.5pt,
						xlabel={Time \,$t$},
						xlabel style={at={(axis description cs:0.5,-0.0)}, anchor=north},
						ylabel={State \,$x\,(t)$},
						ylabel style={at={(axis description cs:-0.08,0.5)}, rotate=90, anchor=south},
						grid=both,
						grid style = {black!20, densely dashed},
						ymin=-30, ymax=10,
						%minor ytick={-8,-4,...,4},
						ytick={-20,-10,0},
						xtick={0,1,...,5},
						%minor xtick={0.5,1.5,2.5},
						tick style={line width=0.8pt},
						extra x ticks={0,0.19,0.44,0.85,2.54},
						extra x tick labels={\only<3->{$t_0$},\only<4->{$t_1$},\only<5->{$t_2$},\only<6->{$t_3$},\only<7->{$t_4$}},
						extra x tick style={tick label style={text=prediction, yshift=1.8cm}, grid=none, tick style={draw=none}},
						set layers=standard,
						axis line style={black!50},
						clip=false
					]


					\addplot[on layer=axis background, no marks, ultra thick] table[x index=0, y index=6, col sep=comma] {../plots/lqr_system_r1.csv};

					\only<3->{
						\addplot[color = prediction, mark=*,  mark size=1.5pt, only marks, ultra thick] coordinates {(0,-26.6524758424985)};}

					\only<4->{
						\addplot[color = prediction, mark=*,  mark size=1.5pt, only marks, ultra thick] coordinates { (0.19,-19.1930968872663) };}

					\only<5->{
						\addplot[color = prediction, mark=*,  mark size=1.5pt, only marks, ultra thick] coordinates {(0.44,-12.1044988669404) };}

					\only<6->{
						\addplot[color = prediction, mark=*,  mark size=1.5pt, only marks, ultra thick] coordinates { (0.85,-5.05679292680688) };}

					\only<7->{
						\addplot[color = prediction, mark=*,  mark size=1.5pt, only marks, ultra thick] coordinates {(2.54, 1.06993515408551)};}

					\only<3->{
						\addplot[prediction, thick, densely dashed, opacity=0.5] coordinates {(0,\tauplace) (0,-19.1930968872663)};}
					\only<4->{
						\addplot[prediction, thick, densely dashed, opacity=0.5] coordinates {(0.19,\tauplace) (0.19,-19.1930968872663)};}
					\only<5->{
						\addplot[prediction, thick, densely dashed, opacity=0.5] coordinates {(0.44,\tauplace) (0.44,-12.1044988669404)};}
					\only<6->{
						\addplot[prediction, thick, densely dashed, opacity=0.5] coordinates {(0.85,\tauplace) (0.85,-5.05679292680688)};}
					\only<7->{
						\addplot[prediction, thick, densely dashed, opacity=0.5] coordinates {(2.54, \tauplace) (2.54, 1.06993515408551)};}

					% Delta lines
					\only<4->{
						\addplot[piecewise, thick] coordinates {(\DeltaLinesX,-26.65) (0,-26.65)};
						\addplot[piecewise, thick] coordinates {(\DeltaLinesX,-19.1930968872663) (0,-19.1930968872663)};}
					\only<5->{
						\addplot[piecewise, thick] coordinates {(\DeltaLinesX,-12.1044988669404) (0,-12.1044988669404)};}
					\only<6->{
						\addplot[piecewise, thick] coordinates {(\DeltaLinesX,-5.05679292680688) (0,-5.05679292680688)};}
					\only<7->{
						\addplot[piecewise, thick] coordinates {(\DeltaLinesX, 1.06993515408551) (0, 1.06993515408551)};}

					% braces for Deltas
					\only<4->{
						\draw[decorate, decoration={brace, mirror}, thick]
						(axis cs:\DeltaX, -26.2) -- (axis cs:\DeltaX, -19.4)
						node[midway, right] {$\Delta$};}

					\only<5->{
						\draw[decorate, decoration={brace, mirror}, thick]
						(axis cs:\DeltaX, -19) -- (axis cs:\DeltaX, -12.2)
						node[midway, right] {$\Delta$};}

				\end{axis}

			\end{tikzpicture}
		\end{example}
	}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Deterministic Lebesgue Sampling: Deriving the Relation for $N$}

	\centering

	\begin{equationbox}[6cm]
		\left\| \,u\,(t_{t+1}) - u(t_k)\, \right\|=\,\Delta
	\end{equationbox}
	\bigskip

	{\raggedright
		In the case of a scalar input ($m=1$) and a given number $N$ of sampling instances in $[0,T]$, the sampling instants $t_k$ satisfy
		\par}
	\newcommand{\LineSpacing}{0mm}

	\begin{eqnarray*}
		\int_{t_k}^{t_{k+1}} \left|\,\dot{u}\,(t)\,\right|\,dt & = & |\,u\,(t_{k+1}) - u\,(t_{k})\,| \, = \, \Delta \\[\LineSpacing]
		\pause & = & \frac{1}{N} \cdot
		\only<1>{\vphantom{\underbrace{\int_{0}^T \, \left|\, \dot{u}\,(s) \,\right| \, ds}_{N\cdot\Delta}}}
		\only<2>{\vphantom{\underbrace{\int_{0}^T \, \left|\, \dot{u}\,(s) \,\right| \, ds}_{N\cdot\Delta}}
		   \int_{0}^T \, \left|\, \dot{u}\,(s) \,\right| \, ds}
		\only<3>{\,\underbrace{\int_{0}^T \, \left|\, \dot{u}\,(s) \,\right| \, ds}_{N\cdot\Delta}}
	\end{eqnarray*}

	\note{\begin{itemize}
			\item we start with the defining equation for lebesgue sampling
			\item we focus on the case of a scalar input, which allows to work on absolute value instead of norm
			\item given number of sampling instances N in the time interval $[0,T]$, we can derive a relation for the sampling instants $t_k$
			\item consider two consecutive sampling instants $t_k$ and $t_{k+1}$
			\item the change of the control input between these two instants is given by the absolute difference $|u(t_{k+1}) - u(t_k)|$
			\item this difference can be written as the integral of the absolute value of the derivative of the control input over the interval from $t_k$ to $t_{k+1}$
			\item this integral represents the total change in the control input over that interval
			\item according to the lebesgue sampling definition, this total change equals the threshold $\Delta$
			\item over $[0,T]$, we have N sampling intervals in total
			\item each of them contributes a change of $\Delta$
			\item so total variation of control over $[0,T]$ is $N\cdot \Delta$
			\item therefore, we can express $\Delta$ as the total change divided by N
			\item Übergang: now that we have this relation, we can use it to derive the sampling density for lebesgue sampling
		\end{itemize}}
\end{frame}

\begin{frame}{Deterministic Lebesgue Sampling: Asymptotic Sampling Density}

	\centering
	\begin{equationbox}[8cm]
		\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt =
		\frac{1}{N} \cdot {\int_{0}^T \, |\, \dot{u}\,(s) \,| \, ds}
	\end{equationbox}
	\vspace{-0.6cm}

	\begin{eqnarray*}
		\sigma_{\text{dls}}\, (t) & = & \lim\limits_{N \to \infty} \frac{1}{N\cdot\tau_k}
		\, = \, \lim\limits_{N \to \infty} \frac{\int_{t_k}^{t_{k+1}} |\,\dot{u}(t)\,|\,dt}
		{\int_{0}^T |\,\dot{u}(s)\,|\,ds}
		\cdot
		\frac{1}{\tau_k} \\[0.4mm]
		\pause
		& = & \lim\limits_{N \to \infty}  \frac{\int_{t_k}^{t_{k+1}} |\,\dot{u}\,(t)\,|\,dt}{\tau_k} \cdot \frac{1}{\int_{0}^T \, |\, \dot{u}\,(s) \,| \, ds} \\[0.4mm]
		\pause
		& = & \lim\limits_{N \to \infty}  \frac{|\,u\,(t_{k+1}) - u\,(t_{k})\,|}{t_{k+1} - t_k} \cdot \frac{1}{\int_{0}^T \, |\, \dot{u}\,(s) \,| \, ds}\\[0.4mm]
		\pause
		&=& \frac{|\, \dot{u}\,(t) \,|}{\int_{0}^T \, |\, \dot{u}\,(s) \,| \, ds}
	\end{eqnarray*}


	\note{\begin{itemize}
			\item we start with the relation we just derived for lebesgue sampling
			\item now we can use this relation to compute the sampling density for lebesgue sampling
			\item we start with the definition of sampling density
			\item we substitute the expression for $N$ from the previous slide into the sampling density formula
			\item we then rearrange the terms
			\item we recognize that the integral from $t_k$ to $t_{k+1}$ is equal to the absolute difference $|u(t_{k+1}) - u(t_k)|$
			\item $\frac{|u(t_{k+1}) - u(t_k)|}{t_{k+1} - t_k}$ is the difference quotient, i.e. the average rate of change of the control input over the interval $[t_k, t_{k+1}]$
			\item By the mean value theorem, this difference quotient equals the derivative $\dot{u}(t)$ at some point t between $t_k$ and $t_{k+1}$.
			\item as $N$ goes to infinity, the interval $[t_k, t_{k+1}]$ shrinks to the point t in our interval, so the difference quotient converges to $\dot{u}(t)$
			\item If $u(t)$ changes quickly at time $t$, $|\dot{u}(t)|$ is large, so the density $\sigma_{\text{dls}}(t)$ is large and more samples are placed there
			\item If $u(t)$ changes slowly, $|\dot{u}(t)|$ is small, so the density is small and fewer samples.
			\item denominator normalizes the density so that it integrates to 1 over $[0,T]$
			\item this final expression shows that the sampling density for lebesgue sampling is proportional to the rate of change of the control input
			\item Unlike periodic sampling, here the inter-sample intervals $\tau_k = t_{k+1} - t_k$ are not constant; they depend on the control signal and vary over time. Because of this, we cannot write down a simple closed-form expression for the normalized cost as we did for periodic sampling. The varying $\tau_k$ prevent us from solving the discrete-time Riccati equation analytically, so we have to rely on numerical evaluation to compute the cost for Lebesgue sampling.
			\item Übergang: next quantization sampling
		\end{itemize}}

\end{frame}
